@misc{madhulatha2012overview,
  title         = {An Overview on Clustering Methods},
  author        = {T. Soni Madhulatha},
  year          = {2012},
  eprint        = {1205.1117},
  archiveprefix = {arXiv},
  primaryclass  = {cs.DS}
}
@article{COVID-19,
  title     = {A novel cluster detection of COVID-19 patients and medical disease conditions using improved evolutionary clustering algorithm star},
  volume    = {138},
  issn      = {0010-4825},
  url       = {http://dx.doi.org/10.1016/j.compbiomed.2021.104866},
  doi       = {10.1016/j.compbiomed.2021.104866},
  journal   = {Computers in Biology and Medicine},
  publisher = {Elsevier BV},
  author    = {Hassan, Bryar A. and Rashid, Tarik A. and Hamarashid, Hozan K.},
  year      = {2021},
  month     = {Nov},
  pages     = {104866}
}
@misc{xing2021learning,
  title         = {Learning Hierarchical Graph Neural Networks for Image Clustering},
  author        = {Yifan Xing and Tong He and Tianjun Xiao and Yongxin Wang and Yuanjun Xiong and Wei Xia and David Wipf and Zheng Zhang and Stefano Soatto},
  year          = {2021},
  eprint        = {2107.01319},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{wang2019linkage,
  title         = {Linkage Based Face Clustering via Graph Convolution Network},
  author        = {Zhongdao Wang and Liang Zheng and Yali Li and Shengjin Wang},
  year          = {2019},
  eprint        = {1903.11306},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@inproceedings{community,
  author    = {Sun, Bing-Jie and Shen, Huawei and Gao, Jinhua and Ouyang, Wentao and Cheng, Xueqi},
  title     = {A Non-Negative Symmetric Encoder-Decoder Approach for Community Detection},
  year      = {2017},
  isbn      = {9781450349185},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3132847.3132902},
  doi       = {10.1145/3132847.3132902},
  abstract  = {Community detection or graph clustering is crucial to understanding the structure of complex networks and extracting relevant knowledge from networked data. Latent factor model, e.g., non-negative matrix factorization and mixed membership block model, is one of the most successful methods for community detection. Latent factor models for community detection aim to find a distributed and generally low-dimensional representation, or coding, that captures the structural regularity of network and reflects the community membership of nodes. Existing latent factor models are mainly based on reconstructing a network from the representation of its nodes, namely network decoder, while constraining the representation to have certain desirable properties. These methods, however, lack an encoder that transforms nodes into their representation. Consequently, they fail to give a clear explanation about the meaning of a community and suffer from undesired computational problems. In this paper, we propose a non-negative symmetric encoder-decoder approach for community detection. By explicitly integrating a decoder and an encoder into a unified loss function, the proposed approach achieves better performance over state-of-the-art latent factor models for community detection task. Moreover, different from existing methods that explicitly impose the sparsity constraint on the representation of nodes, the proposed approach implicitly achieves the sparsity of node representation through its symmetric and non-negative properties, making the optimization much easier than competing methods based on sparse matrix factorization.},
  booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
  pages     = {597–606},
  numpages  = {10},
  keywords  = {latent factor model, encoder-decoder, community detection},
  location  = {Singapore, Singapore},
  series    = {CIKM '17}
}
@misc{luna2021online,
  title         = {Online Clustering-based Multi-Camera Vehicle Tracking in Scenarios with overlapping FOVs},
  author        = {Elena Luna and Juan C. SanMiguel and Jose M. Martínez and Marcos Escudero-Viñolo},
  year          = {2021},
  eprint        = {2102.04091},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@inproceedings{kmeanspp,
  title     = {k-means++: the advantages of careful seeding},
  author    = {David Arthur and Sergei Vassilvitskii},
  booktitle = {SODA '07},
  year      = {2007}
}
@article{Least-squares-quantization-in-PCM,
  author  = {Lloyd, S.},
  journal = {IEEE Transactions on Information Theory},
  title   = {Least squares quantization in PCM},
  year    = {1982},
  volume  = {28},
  number  = {2},
  pages   = {129-137},
  doi     = {10.1109/TIT.1982.1056489}
}
@inproceedings{npc-prove,
  author    = {Mahajan, Meena
               and Nimbhorkar, Prajakta
               and Varadarajan, Kasturi},
  editor    = {Das, Sandip
               and Uehara, Ryuhei},
  title     = {The Planar k-Means Problem is NP-Hard},
  booktitle = {WALCOM: Algorithms and Computation},
  year      = {2009},
  publisher = {Springer Berlin Heidelberg},
  address   = {Berlin, Heidelberg},
  pages     = {274--285},
  abstract  = {In the k-means problem, we are given a finite set S of points in {\$}{\backslash}Re^m{\$}, and integer k{\thinspace}≥{\thinspace}1, and we want to find k points (centers) so as to minimize the sum of the square of the Euclidean distance of each point in S to its nearest center. We show that this well-known problem is NP-hard even for instances in the plane, answering an open question posed by Dasgupta [6].},
  isbn      = {978-3-642-00202-1}
}
@misc{Vattani_thehardness,
  author = {Andrea Vattani},
  title  = {The hardness of k-means clustering in the plane},
  year   = {}
}

@inproceedings{Faster,
  author    = {Khandelwal, Siddhesh
               and Awekar, Amit},
  editor    = {Jose, Joemon M
               and Hauff, Claudia
               and Alt{\i}ngovde, Ismail Sengor
               and Song, Dawei
               and Albakour, Dyaa
               and Watt, Stuart
               and Tait, John},
  title     = {Faster K-Means Cluster Estimation},
  booktitle = {Advances in Information Retrieval},
  year      = {2017},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {520--526},
  abstract  = {There has been considerable work on improving popular clustering algorithm `K-means' in terms of mean squared error (MSE) and speed, both. However, most of the k-means variants tend to compute distance of each data point to each cluster centroid for every iteration. We propose a fast heuristic to overcome this bottleneck with only marginal increase in MSE. We observe that across all iterations of K-means, a data point changes its membership only among a small subset of clusters. Our heuristic predicts such clusters for each data point by looking at nearby clusters after the first iteration of k-means. We augment well known variants of k-means with our heuristic to demonstrate effectiveness of our heuristic. For various synthetic and real-world datasets, our heuristic achieves speed-up of up-to 3 times when compared to efficient variants of k-means.},
  isbn      = {978-3-319-56608-5}
}
@article{LIKAS2003451,
  title    = {The global k-means clustering algorithm},
  journal  = {Pattern Recognition},
  volume   = {36},
  number   = {2},
  pages    = {451-461},
  year     = {2003},
  note     = {Biometrics},
  issn     = {0031-3203},
  doi      = {https://doi.org/10.1016/S0031-3203(02)00060-2},
  url      = {https://www.sciencedirect.com/science/article/pii/S0031320302000602},
  author   = {Aristidis Likas and Nikos Vlassis and Jakob {J. Verbeek}},
  keywords = {Clustering, -Means algorithm, Global optimization, - Trees, Data mining},
  abstract = {We present the global k-means algorithm which is an incremental approach to clustering that dynamically adds one cluster center at a time through a deterministic global search procedure consisting of N (with N being the size of the data set) executions of the k-means algorithm from suitable initial positions. We also propose modifications of the method to reduce the computational load without significantly affecting solution quality. The proposed clustering methods are tested on well-known data sets and they compare favorably to the k-means algorithm with random restarts.}
}
@article{doi:10.1243/095440605X8298,
  author   = {D T Pham;S S Dimov;C D Nguyen;},
  title    = {Selection of <italic>K</italic> in <italic>K</italic>-means clustering},
  journal  = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
  volume   = {219},
  number   = {1},
  pages    = {103-119},
  year     = {2005},
  doi      = {10.1243/095440605X8298},
  url      = {
              http://dx.doi.org/10.1243/095440605X8298
              },
  eprint   = {
              http://dx.doi.org/10.1243/095440605X8298
              },
  abstract = {AbstractThe K-means algorithm is a popular data-clustering algorithm. However, one of its drawbacks is the requirement for the number of clusters, K, to be specified before the algorithm is applied. This paper first reviews existing methods for selecting the number of clusters for the algorithm. Factors that affect this selection are then discussed and a new measure to assist the selection is proposed. The paper concludes with an analysis of the results of using the proposed measure to determine the number of clusters for the K-means algorithm for different data sets.}
}
@article{articleToBound,
  author  = {Elkan, Charles},
  year    = {2003},
  month   = {07},
  pages   = {},
  title   = {Using the Triangle Inequality to Accelerate K-Means},
  volume  = {1},
  journal = {Proceedings, Twentieth International Conference on Machine Learning}
}
@book{datamining,
  author  = {Han, Jiawei and Kamber, Micheline},
  year    = {2000},
  month   = {01},
  pages   = {},
  title   = {Data Mining: Concepts and Techniques},
  isbn    = {1-55860-489-8},
  journal = {San Francisco: Morgan kaufmann;}
}
@article{article,
  author  = {Kothari, Ravi and Pitts, Dax},
  year    = {1999},
  month   = {04},
  pages   = {405-416},
  title   = {On finding the number of clusters},
  volume  = {20},
  journal = {Pattern Recognition Letters},
  doi     = {10.1016/S0167-8655(99)00008-2}
}
@article{article,
  author  = {Peña, José and Lozano, Jose and Larranaga, Pedro},
  year    = {1999},
  month   = {10},
  pages   = {1027-1040},
  title   = {An empirical comparison of four initialization methods for the K-Means algorithm},
  volume  = {20},
  journal = {Pattern Recognition Letters},
  doi     = {10.1016/S0167-8655(99)00069-0}
}
@inproceedings{inproceedings,
  author  = {Hamerly, Greg and Elkan, Charles},
  year    = {2002},
  month   = {01},
  pages   = {600-607},
  title   = {Alternatives to the k-means algorithm that find better clusterings},
  journal = {Proc. of the Int. Conf. on Inf. and Knowledge Manage},
  doi     = {10.1145/584792.584890}
}
@article{article,
  author  = {Kanungo, Tapas and Mount, David and Netanyahu, Nathan and Piatko, Christine and Silverman, Ruth and Wu, Angela},
  year    = {2002},
  month   = {07},
  pages   = {881-892},
  title   = {An Efficient K-Means Clustering Algorithm Analysis and Implementation},
  volume  = {24},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  doi     = {10.1109/TPAMI.2002.1017616}
}
@article{article,
  author  = {Bradley, Paul and Fayyad, Usama},
  year    = {1970},
  month   = {02},
  pages   = {},
  title   = {Refining Initial Points for K-Means Clustering},
  journal = {Refining Initial Points for k-Means Clustering}
}
@article{article,
  author = {Al-daoud, Mohammed and Roberts, Stuart},
  year   = {1995},
  month  = {01},
  pages  = {},
  title  = {New Methods for the Initialisation of Clusters}
}
@article{article,
  author = {Al- Zoubi, Moh'd Belal},
  year   = {1995},
  month  = {07},
  pages  = {},
  title  = {Fast K-MEANS Clustering Algorithms}
}